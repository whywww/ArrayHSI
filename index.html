<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Learned Multi-aperture Color-coded Optics for Snapshot Hyperspectral Imaging</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <link href="https://fonts.cdnfonts.com/css/linux-biolinum" rel="stylesheet">
  <style>
    @import url('https://fonts.cdnfonts.com/css/linux-biolinum');
  </style>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Learned Multi-aperture Color-coded Optics for Snapshot Hyperspectral Imaging</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="" target="_blank">Zheng Shi</a><sup>1,*</sup>,</span>
              <span class="author-block">
                <a href="" target="_blank">Xiong Dun</a><sup>2,*</sup>,</span>
              <span class="author-block">
                <a href="https://whywww.github.io/" target="_blank">Haoyu Wei</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="" target="_blank">Shiyu Dong</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="" target="_blank">Zhanshan Wang</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="" target="_blank">Xinbin Cheng</a><sup>2</sup>,</span><br>
              <span class="author-block">
                <a href="" target="_blank">Felix Heide</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.eee.hku.hk/~evanpeng/" target="_blank">Yifan (Evan) Peng</a><sup>3</sup></span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Princeton University, <sup>2</sup>Tongji University, <sup>3</sup>The University of Hong Kong<br><b>ACM Transactions on Graphics, 2024</b></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded button-color">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="" target="_blank"
                      class="external-link button is-normal is-rounded button-color">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="" target="_blank"
                    class="external-link button is-normal is-rounded button-color">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img src="static/images/teaser_2024_V5.jpg" alt="MY ALT TEXT" class="abs-image"/>
      <h2 class="subtitle has-text-centered">
        We propose a snapshot hyperspectral imager with multi-aperture color-coded optics, illustrated in (a/e), providing customized independent spatial and spectral encoding for different channels, as shown in (f/g). We achieve this by jointly optimizing a DOE array, aperture-wise color filters, and a reconstruction network. This approach exploits the degrees of freedom in optical encoding across both spatial and spectral dimensions, outperforming existing single-lens approaches by over 5 dB PSNR in reconstruction quality. We experimentally validate the proposed method in both indoor and outdoor settings, recovering up to 31 spectral bands within the 429–700 nm range, closely matching the reference captured by a spectral-scan hyperspectral camera, as shown in (b-d).
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Learned optics, which incorporate lightweight diffractive optics, coded-aperture modulation, and specialized image-processing neural networks, have recently garnered attention in the field of snapshot hyperspectral imaging (HSI). While conventional methods typically rely on a single lens element paired with an off-the-shelf color sensor, these setups, despite their widespread availability, present inherent limitations. First, the Bayer sensor's spectral response curves are not optimized for HSI applications, limiting spectral fidelity of the reconstruction. Second, single lens designs rely on a single diffractive optical element (DOE)  to simultaneously encode spectral information and maintain spatial resolution across all wavelengths, which constrains spectral encoding capabilities.
This work investigates a multi-channel lens array combined with aperture-wise color filters, all co-optimized alongside an image reconstruction network. This configuration enables independent spatial encoding and spectral response for each channel, improving optical encoding across both spatial and spectral dimensions. Specifically, we validate that the method achieves over a 5dB improvement in PSNR for spectral reconstruction compared to existing single-diffractive lens and coded-aperture techniques. Experimental validation further confirmed that the method is capable of recovering up to 31 spectral bands within the 429–700 nm range in diverse indoor and outdoor environments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- <div class="hero-body">
  <div class="container">
    <div class="item">
      <img src="static/images/teaser_2024_V5.png" alt="MY ALT TEXT" class="abs-image"/>
      <h2 class="subtitle has-text-centered">
        Diagram of off-axis diffraction.
      </h2>
    </div>
  </div>
</div>  -->

<!-- End paper abstract -->


<section class="section is-compact">
  <div class="container is-max-desktop content">
    <p></p>
    <h2 class="title">System Overview</h2>
      <div class="item">
        <p>
          We jointly optimize the multi-aperture DOE array, aperture-wise color filters, and image reconstruction network using a hybrid loss function. During each forward pass, the ground truth spectral images are first convolved with the PSFs of the DOE array and then multiplied by the response curves of the color filters. Noise is added to the simulated sensor image, which is then integrated over the monochrome sensor's response for each sub-lens channel: B, G1, G2, and R. These images are input into the multi-resolution feature extractor of the image reconstruction network to recover the final hyperspectral (HS) and RGB images.
        </p>
        <img src="static/images/overview.jpg" class="body-image"/>
        <div class="subtitle has-text-centered">
          Learning Multi-Aperture Color-Coded Optics for Snapshot Hyperspectral Imaging.
        </div>
        <!-- <p>where<math xmlns="http://www.w3.org/1998/Math/MathML" style="color:rgba(0,0,0,255);color:rgb(0,0,0);font-size:13.00pt;"><mstyle><mrow><mtable columnspacing="0.1em" columnalign="left" displaystyle="true"><mtr><mtd><mrow><msub><mi>H</mi><mi>z</mi></msub></mrow></mtd></mtr></mtable></mrow></mstyle></math>is the propagation function.
          <p>We propose the Least-Sampling ASM (LS-ASM) that minimizes the sampling requirements in both spatial and frequency domains and unifies the critical sampling rates for all types of input fields. </p>
          <ol>
          <li>With <i>linear phase compensation (LPC)</i>, we shift the frequency centers of off-axis wave fields to the origin using the Fourier transform properties;</li>
          <li>With <i>virtual lens model</i>, we calculate a controllable region of band extension as a supplement to the phase gradient analysis;</li>
          <li>With <i>joint component analysis</i>, we combine different components in the same domain to determine the correct sampling rate.</li>
          </ol>
        </p> -->
        <!-- <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <img src="static/images/lpc.png" alt="MY ALT TEXT" class="sec-image"/>
            <div class="subtitle has-text-centered">
              The Linear Phase Compensation (LPC) shifts the angular spectrum of the input field to the frequency center. 
            </div>
          </div>
          <div class="item">
            <img src="static/images/effective_bandwidth.png" alt="MY ALT TEXT" class="sec-image"/>
            <div class="subtitle has-text-centered">
              (a) Fourier transform interpreted as a virtual thin lens model. The green and yellow areas denote spherical and plane waves, respectively. (b) Inﬂuence of oversampling factor.
            </div>
          </div>
          <div class="item">
            <img src="static/images/combined-sampling.png" alt="MY ALT TEXT" class="sec-image"/>
            <div class="subtitle has-text-centered">
              Effect of combined sampling in frequency domain. From left to right, the 1st column is the zoomed view from the right ones. The 2nd ~ 4th ones are the phase of the angular spectrum of an off-axis converging spherical wave with LPC, the shifted transfer function, and the superposition, respectively.
            </div>
          </div>
        </div> -->
        <!-- <p></p> -->
        <!-- <p>
          We summarize the complete minimum sampling rates in both domains as:
        </p>
        <img src="static/images/spat.png" class="eq-image"/>
        <img src="static/images/freq.png" class="eq-image"/> -->
      </div>
</section>

<section class="section is-compact"></section>
  <div class="container is-max-desktop content">
    <h2 class="title">System Analysis</h2>
    <p>We assess the benefits of the proposed multi-aperture setup by analyzing the performance enhancements from spatial and spectral modulation, as well as the effects of independent versus shared spatial modulation across channels. To this end, we compare the proposed approach to variants using a fixed Bayer RGGB color filter and/or a single shared DOE across all color channels.
      We observe a noticeable decline in both RGB and hyperspectral reconstruction quality when compared to the proposed multi-aperture configuration.
      We also confirm that Bayer filters, designed to mimic human visual perception, are not tailored to HSI applications, leading to a performance decline compared to our customized color filter.
    </p>
      <div class="item">
        <img src="static/images/ablation_array.png" alt="MY ALT TEXT" class="sec-image"/>
        <div class="subtitle has-text-centered">
          Validation of Multi-Aperture Configuration.
        </div>
      </div>
    </div>
</section>

<section class="section is-compact">
  <div class="container is-max-desktop content">
    <h2 class="title">Experimental Results</h2>
    <p>We validate the proposed system under both outdoor and indoor environments and compare the reconstruction with reference captures obtained from the commercially available Specim IQ hyperspectral camera. 
      Spectral curves reconstructed by our method closely align with those from the Specim IQ camera, demonstrating exceptional fidelity across 31 channels, further validating our methodology in diverse environments.
    </p>
      <div class="item">
        <img src="static/images/experimental.jpg" alt="MY ALT TEXT" class="body-image"/>
        <div class="subtitle has-text-centered">
          <b>Experimental Assessment.</b> We evaluate the proposed method in both outdoor (Scene 1 and 2) and indoor (Scene 3) environments, comparing it to the commercial Specim IQ hyperspectral camera. For each scene, we include: (a) sensor captures comprising four sub-channel images (R, G1, G2, B); (b/e) RGB reconstructions compared to Specim IQ references; (c/f) close-up views of a cropped region across all 31 channels; and (d) spectral validation plots for four sampled points on the captured scene.
        </div>
      </div>
    </div>
</section>


<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
      <div class="item">
        <img src="static/images/results.png" alt="MY ALT TEXT" class="abs-image"/>
        <h2 class="subtitle has-text-centered">
          (a) Visual results of the amplitude (first row) and phase (second row) of the complex PSF, and the amplitude for the angular spectrum (third row) of the input field at 3 degrees. The amplitude of the PSF and the angular spectrum is normalized by respective maxima. In the third row, the red dots denote the center of the angular spectrum. (b) SNR and runtime w.r.t. incident angles. (c) Sampling number w.r.t. incident angles. 
        </h2>
      </div>
      <div class="item">
        <img src="static/images/uniform-diffuser.png" alt="MY ALT TEXT" class="abs-image"/>
        <h2 class="subtitle has-text-centered">
          Complex diffractive fields modulated by random diffusers with uniform distributions within each pixel. Here the phase modulation is wrapped to [0, 2π] for visualization.
       </h2>
     </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->




<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
<section class="section is-compact" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      
    </code></pre>
  </div>
</section>
<!--End BibTex citation -->


<!-- Related projects -->
<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title">Related Projects</h2>
    You may also be interested in related projects in deep optics:
    <ul>
      <li>Liu et al. Deep Optics Models, Optics Express 2022 (<a href="https://opg.optica.org/oe/fulltext.cfm?uri=oe-30-20-36973&id=506502">link</a>)</li>
      <!-- <li>Ikoma et al. Depth from Defocus, ICCP 2021 (<a href="https://ieeexplore.ieee.org/abstract/document/9466261/">link</a>)</li> -->
      <li>Dun et al. Rotationally Symmetric Achromat, Optica 2020 (<a href="https://opg.optica.org/optica/fulltext.cfm?uri=optica-7-8-913&id=433999">link</a>)</li>
      <li>Metzler et al. Deep Optics for HDR Imaging, CVPR 2020 (<a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Metzler_Deep_Optics_for_Single-Shot_High-Dynamic-Range_Imaging_CVPR_2020_paper.html">link</a>)</li>
      <!-- <li>Peng et al. Neural Holography, ACM ToG 2020 (<a href="https://dl.acm.org/doi/abs/10.1145/3414685.3417802">link</a>)</li> -->
    </ul>
  </div>
</section>
<!-- End related projects -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            This content is released under the Creative Commons Attribution-NonCommercial 4.0 International license (CC BY-NC.) 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->
<!-- Default Statcounter code for My Personal Website https://whywww.github.io/ -->
<script type="text/javascript">
  var sc_project=12906744; 
  var sc_invisible=1; 
  var sc_security="5b484c07"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript><div class="statcounter"><a title="Web Analytics"
  href="https://statcounter.com/" target="_blank"><img class="statcounter"
  src="https://c.statcounter.com/12906744/0/5b484c07/1/" alt="Web Analytics"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
<!-- End of Statcounter Code -->

  </body>
  </html>
